{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### María Sofía Álvarez - Brenda Barahona - Álvaro Plata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>Proyecto 1: Analítica de textos</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación de librerías\n",
    "<b><font color='blue'>Importante:</font></b> Correr antes de ejecutar el notebook. Con una única vez que se corra, basta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/neomatrix369/nlp_profiler.git@master\n",
      "  Cloning https://github.com/neomatrix369/nlp_profiler.git (to revision master) to c:\\users\\alvaro\\appdata\\local\\temp\\pip-req-build-ymb6cqe3\n",
      "  Resolved https://github.com/neomatrix369/nlp_profiler.git to commit bde13ee6958d4efc48366c86ab952545b621eff7\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting en-core-web-sm@ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.0/en_core_web_sm-2.3.0.tar.gz\n",
      "  Using cached en_core_web_sm-2.3.0-py3-none-any.whl\n",
      "Requirement already satisfied: language-tool-python>=2.6.1 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nlp-profiler==0.0.3) (2.7.0)\n",
      "Requirement already satisfied: pandas<1.3.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nlp-profiler==0.0.3) (1.2.0)\n",
      "Requirement already satisfied: ipython>=7.12.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nlp-profiler==0.0.3) (7.29.0)\n",
      "Requirement already satisfied: spacy<3.0.0,>=2.3.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nlp-profiler==0.0.3) (2.3.7)\n",
      "Requirement already satisfied: emoji>=0.5.4 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nlp-profiler==0.0.3) (1.7.0)\n",
      "Requirement already satisfied: joblib>=0.14.1 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nlp-profiler==0.0.3) (1.0.1)\n",
      "Requirement already satisfied: textblob>=0.15.3 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nlp-profiler==0.0.3) (0.17.1)\n",
      "Requirement already satisfied: textstat>=0.7.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nlp-profiler==0.0.3) (0.7.3)\n",
      "Requirement already satisfied: swifter>=1.0.3 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nlp-profiler==0.0.3) (1.1.2)\n",
      "Requirement already satisfied: tqdm==4.46.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nlp-profiler==0.0.3) (4.46.0)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nlp-profiler==0.0.3) (2.26.0)\n",
      "Requirement already satisfied: nltk>=3.5 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nlp-profiler==0.0.3) (3.6.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipython>=7.12.0->nlp-profiler==0.0.3) (3.0.20)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipython>=7.12.0->nlp-profiler==0.0.3) (0.18.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipython>=7.12.0->nlp-profiler==0.0.3) (5.1.0)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipython>=7.12.0->nlp-profiler==0.0.3) (0.1.2)\n",
      "Requirement already satisfied: pygments in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipython>=7.12.0->nlp-profiler==0.0.3) (2.10.0)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipython>=7.12.0->nlp-profiler==0.0.3) (0.7.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipython>=7.12.0->nlp-profiler==0.0.3) (0.4.4)\n",
      "Requirement already satisfied: decorator in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipython>=7.12.0->nlp-profiler==0.0.3) (5.1.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipython>=7.12.0->nlp-profiler==0.0.3) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipython>=7.12.0->nlp-profiler==0.0.3) (58.0.4)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=7.12.0->nlp-profiler==0.0.3) (0.8.2)\n",
      "Requirement already satisfied: click in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nltk>=3.5->nlp-profiler==0.0.3) (8.0.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nltk>=3.5->nlp-profiler==0.0.3) (2021.8.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from pandas<1.3.0->nlp-profiler==0.0.3) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from pandas<1.3.0->nlp-profiler==0.0.3) (1.20.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from pandas<1.3.0->nlp-profiler==0.0.3) (2.8.2)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.12.0->nlp-profiler==0.0.3) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas<1.3.0->nlp-profiler==0.0.3) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from requests>=2.23.0->nlp-profiler==0.0.3) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from requests>=2.23.0->nlp-profiler==0.0.3) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from requests>=2.23.0->nlp-profiler==0.0.3) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from requests>=2.23.0->nlp-profiler==0.0.3) (2.0.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from spacy<3.0.0,>=2.3.0->nlp-profiler==0.0.3) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from spacy<3.0.0,>=2.3.0->nlp-profiler==0.0.3) (2.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from spacy<3.0.0,>=2.3.0->nlp-profiler==0.0.3) (0.7.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from spacy<3.0.0,>=2.3.0->nlp-profiler==0.0.3) (1.0.6)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from spacy<3.0.0,>=2.3.0->nlp-profiler==0.0.3) (1.0.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from spacy<3.0.0,>=2.3.0->nlp-profiler==0.0.3) (0.9.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from spacy<3.0.0,>=2.3.0->nlp-profiler==0.0.3) (1.0.5)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from spacy<3.0.0,>=2.3.0->nlp-profiler==0.0.3) (7.4.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from spacy<3.0.0,>=2.3.0->nlp-profiler==0.0.3) (3.0.6)\n",
      "Requirement already satisfied: bleach>=3.1.1 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from swifter>=1.0.3->nlp-profiler==0.0.3) (4.0.0)\n",
      "Requirement already satisfied: psutil>=5.6.6 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from swifter>=1.0.3->nlp-profiler==0.0.3) (5.8.0)\n",
      "Requirement already satisfied: cloudpickle>=0.2.2 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from swifter>=1.0.3->nlp-profiler==0.0.3) (2.0.0)\n",
      "Requirement already satisfied: ipywidgets>=7.0.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from swifter>=1.0.3->nlp-profiler==0.0.3) (7.6.5)\n",
      "Requirement already satisfied: dask[dataframe]>=2.10.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from swifter>=1.0.3->nlp-profiler==0.0.3) (2021.10.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from bleach>=3.1.1->swifter>=1.0.3->nlp-profiler==0.0.3) (0.5.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from bleach>=3.1.1->swifter>=1.0.3->nlp-profiler==0.0.3) (21.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter>=1.0.3->nlp-profiler==0.0.3) (2021.10.1)\n",
      "Requirement already satisfied: partd>=0.3.10 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter>=1.0.3->nlp-profiler==0.0.3) (1.2.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter>=1.0.3->nlp-profiler==0.0.3) (6.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter>=1.0.3->nlp-profiler==0.0.3) (0.11.1)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (0.2.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (1.0.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (3.5.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (6.4.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (5.1.3)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (1.4.1)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (6.1)\n",
      "Requirement already satisfied: jupyter-client<8.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (6.1.12)\n",
      "Requirement already satisfied: pyzmq>=13 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (22.2.1)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (4.8.1)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from jupyter-core>=4.6.0->jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (228)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (3.2.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (21.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (0.18.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from packaging->bleach>=3.1.1->swifter>=1.0.3->nlp-profiler==0.0.3) (3.0.4)\n",
      "Requirement already satisfied: locket in c:\\users\\alvaro\\anaconda3\\lib\\site-packages\\locket-0.2.1-py3.9.egg (from partd>=0.3.10->dask[dataframe]>=2.10.0->swifter>=1.0.3->nlp-profiler==0.0.3) (0.2.1)\n",
      "Requirement already satisfied: pyphen in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from textstat>=0.7.0->nlp-profiler==0.0.3) (0.12.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (6.4.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (2.11.3)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (0.11.0)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (6.1.0)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (20.1.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (0.9.4)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (1.8.0)\n",
      "Requirement already satisfied: pywinpty>=0.5 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from terminado>=0.8.3->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (0.5.7)\n",
      "Requirement already satisfied: cffi>=1.0.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (1.14.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (2.20)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (2.0.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (0.1.2)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (0.7.1)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (0.3)\n",
      "Requirement already satisfied: testpath in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (0.5.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (0.8.4)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (0.5.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (1.4.3)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (1.5.1)\n",
      "Requirement already satisfied: async-generator in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (1.10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/neomatrix369/nlp_profiler.git@master\n",
      "  Cloning https://github.com/neomatrix369/nlp_profiler.git (to revision master) to c:\\users\\alvaro\\appdata\\local\\temp\\pip-req-build-ymb6cqe3\n",
      "  Resolved https://github.com/neomatrix369/nlp_profiler.git to commit bde13ee6958d4efc48366c86ab952545b621eff7\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting en-core-web-sm@ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.0/en_core_web_sm-2.3.0.tar.gz\n",
      "  Using cached en_core_web_sm-2.3.0-py3-none-any.whl\n",
      "Requirement already satisfied: language-tool-python>=2.6.1 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nlp-profiler==0.0.3) (2.7.0)\n",
      "Requirement already satisfied: pandas<1.3.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nlp-profiler==0.0.3) (1.2.0)\n",
      "Requirement already satisfied: ipython>=7.12.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nlp-profiler==0.0.3) (7.29.0)\n",
      "Requirement already satisfied: spacy<3.0.0,>=2.3.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nlp-profiler==0.0.3) (2.3.7)\n",
      "Requirement already satisfied: emoji>=0.5.4 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nlp-profiler==0.0.3) (1.7.0)\n",
      "Requirement already satisfied: joblib>=0.14.1 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nlp-profiler==0.0.3) (1.0.1)\n",
      "Requirement already satisfied: textblob>=0.15.3 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nlp-profiler==0.0.3) (0.17.1)\n",
      "Requirement already satisfied: textstat>=0.7.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nlp-profiler==0.0.3) (0.7.3)\n",
      "Requirement already satisfied: swifter>=1.0.3 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nlp-profiler==0.0.3) (1.1.2)\n",
      "Requirement already satisfied: tqdm==4.46.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nlp-profiler==0.0.3) (4.46.0)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nlp-profiler==0.0.3) (2.26.0)\n",
      "Requirement already satisfied: nltk>=3.5 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nlp-profiler==0.0.3) (3.6.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipython>=7.12.0->nlp-profiler==0.0.3) (3.0.20)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipython>=7.12.0->nlp-profiler==0.0.3) (0.18.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipython>=7.12.0->nlp-profiler==0.0.3) (5.1.0)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipython>=7.12.0->nlp-profiler==0.0.3) (0.1.2)\n",
      "Requirement already satisfied: pygments in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipython>=7.12.0->nlp-profiler==0.0.3) (2.10.0)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipython>=7.12.0->nlp-profiler==0.0.3) (0.7.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipython>=7.12.0->nlp-profiler==0.0.3) (0.4.4)\n",
      "Requirement already satisfied: decorator in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipython>=7.12.0->nlp-profiler==0.0.3) (5.1.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipython>=7.12.0->nlp-profiler==0.0.3) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipython>=7.12.0->nlp-profiler==0.0.3) (58.0.4)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=7.12.0->nlp-profiler==0.0.3) (0.8.2)\n",
      "Requirement already satisfied: click in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nltk>=3.5->nlp-profiler==0.0.3) (8.0.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nltk>=3.5->nlp-profiler==0.0.3) (2021.8.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from pandas<1.3.0->nlp-profiler==0.0.3) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from pandas<1.3.0->nlp-profiler==0.0.3) (1.20.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from pandas<1.3.0->nlp-profiler==0.0.3) (2.8.2)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.12.0->nlp-profiler==0.0.3) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas<1.3.0->nlp-profiler==0.0.3) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from requests>=2.23.0->nlp-profiler==0.0.3) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from requests>=2.23.0->nlp-profiler==0.0.3) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from requests>=2.23.0->nlp-profiler==0.0.3) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from requests>=2.23.0->nlp-profiler==0.0.3) (2.0.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from spacy<3.0.0,>=2.3.0->nlp-profiler==0.0.3) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from spacy<3.0.0,>=2.3.0->nlp-profiler==0.0.3) (2.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from spacy<3.0.0,>=2.3.0->nlp-profiler==0.0.3) (0.7.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from spacy<3.0.0,>=2.3.0->nlp-profiler==0.0.3) (1.0.6)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from spacy<3.0.0,>=2.3.0->nlp-profiler==0.0.3) (1.0.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from spacy<3.0.0,>=2.3.0->nlp-profiler==0.0.3) (0.9.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from spacy<3.0.0,>=2.3.0->nlp-profiler==0.0.3) (1.0.5)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from spacy<3.0.0,>=2.3.0->nlp-profiler==0.0.3) (7.4.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from spacy<3.0.0,>=2.3.0->nlp-profiler==0.0.3) (3.0.6)\n",
      "Requirement already satisfied: bleach>=3.1.1 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from swifter>=1.0.3->nlp-profiler==0.0.3) (4.0.0)\n",
      "Requirement already satisfied: psutil>=5.6.6 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from swifter>=1.0.3->nlp-profiler==0.0.3) (5.8.0)\n",
      "Requirement already satisfied: cloudpickle>=0.2.2 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from swifter>=1.0.3->nlp-profiler==0.0.3) (2.0.0)\n",
      "Requirement already satisfied: ipywidgets>=7.0.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from swifter>=1.0.3->nlp-profiler==0.0.3) (7.6.5)\n",
      "Requirement already satisfied: dask[dataframe]>=2.10.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from swifter>=1.0.3->nlp-profiler==0.0.3) (2021.10.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from bleach>=3.1.1->swifter>=1.0.3->nlp-profiler==0.0.3) (0.5.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from bleach>=3.1.1->swifter>=1.0.3->nlp-profiler==0.0.3) (21.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter>=1.0.3->nlp-profiler==0.0.3) (2021.10.1)\n",
      "Requirement already satisfied: partd>=0.3.10 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter>=1.0.3->nlp-profiler==0.0.3) (1.2.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter>=1.0.3->nlp-profiler==0.0.3) (6.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter>=1.0.3->nlp-profiler==0.0.3) (0.11.1)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (0.2.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (1.0.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (3.5.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (6.4.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (5.1.3)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (1.4.1)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (6.1)\n",
      "Requirement already satisfied: jupyter-client<8.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (6.1.12)\n",
      "Requirement already satisfied: pyzmq>=13 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (22.2.1)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (4.8.1)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from jupyter-core>=4.6.0->jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (228)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (3.2.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (21.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (0.18.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from packaging->bleach>=3.1.1->swifter>=1.0.3->nlp-profiler==0.0.3) (3.0.4)\n",
      "Requirement already satisfied: locket in c:\\users\\alvaro\\anaconda3\\lib\\site-packages\\locket-0.2.1-py3.9.egg (from partd>=0.3.10->dask[dataframe]>=2.10.0->swifter>=1.0.3->nlp-profiler==0.0.3) (0.2.1)\n",
      "Requirement already satisfied: pyphen in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from textstat>=0.7.0->nlp-profiler==0.0.3) (0.12.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (6.4.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (2.11.3)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (0.11.0)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (6.1.0)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (20.1.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (0.9.4)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (1.8.0)\n",
      "Requirement already satisfied: pywinpty>=0.5 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from terminado>=0.8.3->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (0.5.7)\n",
      "Requirement already satisfied: cffi>=1.0.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (1.14.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (2.20)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (2.0.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (0.1.2)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (0.7.1)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (0.3)\n",
      "Requirement already satisfied: testpath in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (0.5.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (0.8.4)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (0.5.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (1.4.3)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (1.5.1)\n",
      "Requirement already satisfied: async-generator in c:\\users\\alvaro\\anaconda3\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter>=1.0.3->nlp-profiler==0.0.3) (1.10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/neomatrix369/nlp_profiler.git 'C:\\Users\\Alvaro\\AppData\\Local\\Temp\\pip-req-build-ymb6cqe3'\n",
      "  Running command git clone -q https://github.com/neomatrix369/nlp_profiler.git 'C:\\Users\\Alvaro\\AppData\\Local\\Temp\\pip-req-build-ymb6cqe3'\n"
     ]
    }
   ],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "!pip install -U git+https://github.com/neomatrix369/nlp_profiler.git@master\n",
    "#!pip install wordcloud\n",
    "#!pip install contractions\n",
    "#!pip install nltk\n",
    "#!pip install inflect\n",
    "#!pip install unicode\n",
    "#!pip install stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de librerías\n",
    "Importamos las librerías necesarias para el desarrollo de este proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Alvaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alvaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Alvaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Alvaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alvaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Alvaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No java install detected. Please install java to use language-tool-python.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14564/4145899518.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#from wordcloud import WordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnlp_profiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapply_text_profiling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Para resampling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nlp_profiler\\core.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnlp_profiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgranular_features\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapply_granular_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnlp_profiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhigh_level_features\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapply_high_level_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnlp_profiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhigh_level_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrammar_quality_check\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mapply_grammar_check\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnlp_profiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhigh_level_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspelling_quality_check\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nlp_profiler\\high_level_features\\grammar_quality_check.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlanguage_tool_python\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlanguage_tool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlanguage_tool_python\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLanguageTool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en-GB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\language_tool_python\\server.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, language, motherTongue, remote_server, newSpellings, new_spellings_persist, host, config)\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_remote_server_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_server_is_alive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_server_on_free_port\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\language_tool_python\\server.py\u001b[0m in \u001b[0;36m_start_server_on_free_port\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'http://{}:{}/v2/'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_host\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_port\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_local_server\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mServerError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\language_tool_python\\server.py\u001b[0m in \u001b[0;36m_start_local_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_start_local_server\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;31m# Before starting local server, download language tool if needed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m         \u001b[0mdownload_lt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m         \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\language_tool_python\\download_lt.py\u001b[0m in \u001b[0;36mdownload_lt\u001b[1;34m(update)\u001b[0m\n\u001b[0;32m    142\u001b[0m     ]\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[0mconfirm_java_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m     \u001b[0mversion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLATEST_VERSION\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFILENAME\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\language_tool_python\\download_lt.py\u001b[0m in \u001b[0;36mconfirm_java_compatibility\u001b[1;34m()\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;31m# found because of a PATHEXT-related issue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;31m# (https://bugs.python.org/issue2200).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No java install detected. Please install java to use language-tool-python.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     output = subprocess.check_output([java_path, '-version'],\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No java install detected. Please install java to use language-tool-python."
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No java install detected. Please install java to use language-tool-python.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14564/4145899518.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#from wordcloud import WordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnlp_profiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapply_text_profiling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Para resampling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nlp_profiler\\core.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnlp_profiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgranular_features\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapply_granular_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnlp_profiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhigh_level_features\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapply_high_level_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnlp_profiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhigh_level_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrammar_quality_check\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mapply_grammar_check\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnlp_profiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhigh_level_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspelling_quality_check\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nlp_profiler\\high_level_features\\grammar_quality_check.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlanguage_tool_python\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlanguage_tool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlanguage_tool_python\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLanguageTool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en-GB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\language_tool_python\\server.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, language, motherTongue, remote_server, newSpellings, new_spellings_persist, host, config)\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_remote_server_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_server_is_alive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_server_on_free_port\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\language_tool_python\\server.py\u001b[0m in \u001b[0;36m_start_server_on_free_port\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'http://{}:{}/v2/'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_host\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_port\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_local_server\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mServerError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\language_tool_python\\server.py\u001b[0m in \u001b[0;36m_start_local_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_start_local_server\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;31m# Before starting local server, download language tool if needed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m         \u001b[0mdownload_lt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m         \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\language_tool_python\\download_lt.py\u001b[0m in \u001b[0;36mdownload_lt\u001b[1;34m(update)\u001b[0m\n\u001b[0;32m    142\u001b[0m     ]\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[0mconfirm_java_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m     \u001b[0mversion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLATEST_VERSION\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFILENAME\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\language_tool_python\\download_lt.py\u001b[0m in \u001b[0;36mconfirm_java_compatibility\u001b[1;34m()\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;31m# found because of a PATHEXT-related issue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;31m# (https://bugs.python.org/issue2200).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No java install detected. Please install java to use language-tool-python.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     output = subprocess.check_output([java_path, '-version'],\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No java install detected. Please install java to use language-tool-python."
     ]
    }
   ],
   "source": [
    "# ESAI\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_profiling as pp\n",
    "import matplotlib.pyplot as plt\n",
    "#from wordcloud import WordCloud\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nlp_profiler.core import apply_text_profiling\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# Para resampling\n",
    "from sklearn.utils import resample \n",
    "import contractions\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import inflect\n",
    "import re\n",
    "import unicodedata\n",
    "import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las historias médicas describen las condiciones actuales de un paciente. Los médicos rutinariamente escanean docenas o cientos de historias clínicas en un solo día mientras hacen sus turnos en un hospital y deben resaltar la información relevante de ellos para poder determinar la enfermedad que padece un paciente. El objetivo de este proyecto es, a partir de dichas historias clínicas dadas por los médicos, crear una herramienta que ayude en la identificación del problema/enfermedad que un paciente padece. Se busca que las predicciones realizadas sean bastante precisas, pues los pacientes deben ser tratados acorde con la enfermedad que padecen. \n",
    "\n",
    "A continuación, se leerá el conjunto de datos correspondiente. Este incluye historias clínicas de varios pacientes, con la información más relevante de sus enfermedades reportada por los médicos. Asimismo, para cada paciente se tiene la enfermedad que padece. De acuerdo con el diccionario, esta puede ser cualquiera de las siguientes 5 categorías: \n",
    "1. Neoplasms (Neoplasias).\n",
    "2. Digestive system diseases (Enfermedades del sistema digestivo).\n",
    "3. Nervous system diseases (Enfermedades del sistema nervioso).\n",
    "4. Cardiovascular diseases (Enfermedades cardiovasculares).\n",
    "5. General pathological conditions (Condiciones patológicas generales).\n",
    "\n",
    "Es posible ver que se tiene un problema de clasificación multiclase. \n",
    "\n",
    "---\n",
    "## 1. Entendimiento del problema\n",
    "Lo primero que hacemos es cargar las librerías y el conjunto de datos, con el fin de entender las particularidades del problema que estamos enfrentando.\n",
    "### 1.1 Perfilamiento de los datos\n",
    "Procedemos, entonces, a ver los datos suministrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnoses =pd.read_csv('ApoyoDiagnosticoEstudiante/medical_text_clasificacion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tamaño del conjunto de datos:\", len(diagnoses))\n",
    "diagnoses.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto, podemos ver una muestra de los datos. Aquí, nos damos cuenta que nuestro conjunto de datos consiste de dos columnas: \"medical_abstracts\", con las historias clínicas de los pacientes (escritas en inglés), y \"problems_described\", que es un número con la enfermedad padecida por el paciente, de acuerdo con lo descrito arriba y en concordancia con el diccionario.\n",
    "\n",
    "Asimismo, vemos que tenemos 12000 historias clínicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Las categorías de las enfermedades son: {}'.format(sorted(diagnoses['problems_described'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que podemos revisar es si hay algún problema que no se encuentre en las categorías 1-5 definidas para las enfermedades. Como vemos, en este caso todas las categorías concuerdan con las definidas por el negocio. Asimismo, nos damos cuenta que la clasificación es multiclase y no multietiqueta: cada historia clínica tiene una única enfermedad asociada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora queremos ver si nuestro conjunto de datos tiene entradas nulas. Para ello, revisamos si hay historias clínicas que correspondan con la cadena vacía \"\" o con un sólo espacio \" \". Se ve que no hay ninguna con estas características, pues obtenemos un <i>dataframe</i> vacío, como vemos en la celda a continuación. Podemos hacer también una revisión con el método <code>isna()</code> de <code>pandas</code>. Este indica si hay algún valor de los datos que tenga como valor <code>None</code> o <code>numpy.NaN</code>. Vemos entonces que nuestro conjunto de datos no tiene ni nulos, ni faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diagnoses.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacias = diagnoses[(diagnoses['medical_abstracts'] == \"\") | (diagnoses['medical_abstracts'] == \" \")]\n",
    "print('La cantidad de de historias clínicas vacías es: {}'.format(len(vacias)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asimismo, confirmamos que no hay historias clínicas duplicadas en el conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicados = diagnoses[diagnoses.duplicated()]\n",
    "print('Número de registros duplicados: {}'.format(len(duplicados)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos entonces que la calidad de los datos, en el sentido de nulos y duplicados, es bastante buena.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos damos cuenta ahora de que sólo tenemos una variable explicativa (la historia clínica) y que la variable objetivo (target) es la enfermedad que padece el usuario. Así las cosas partimos el conjunto de datos entre los datos de entrenamiento y de test: esto es crucial, pues, tras entrenar el modelo, debemos validarlo con datos que este nunca había visto. De acuerdo con el libro de Géron <a href='#geron'>[1]</a>, una buena división entre datos de entrenamiento y test es 80% y 20%, respectivamente, y en consecuencia es la que usaremos para toda esta exploración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = diagnoses['medical_abstracts'], diagnoses['problems_described']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,stratify=Y,test_size=0.2, random_state=28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto, procedemos a hacer todo el entrenamiento sobre el conjunto de $X_{\\mathrm{train}},Y_{\\mathrm{train}}$ y guardamos el test en la \"caja fuerte\" hasta que sea el momento de la validación.\n",
    "\n",
    "Queremos ver la distribución de las clases de nuestro conjunto de datos. Para ello, consideremos el siguiente histograma y la distribución de los textos de acuerdo con sus clases:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanceo de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_counts_y = Y_train.value_counts()\n",
    "plt.bar(range(len(val_counts_y)), val_counts_y.values, align='center',color='skyblue')\n",
    "plt.xticks(range(len(val_counts_y)), val_counts_y.index.values, size='small')\n",
    "plt.title('División de las clases del conjunto de datos')\n",
    "plt.show()\n",
    "\n",
    "print('Veamos la cantidad de datos en cada clase: ')\n",
    "Y_train.value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver claramente que las clases están notablemente desbalanceadas. Vemos que la clase con más datos es la 5, lo cual es consecuente con el hecho de que esta es la clase más amplia, al tratar condiciones patológicas generales. Debemos clasificar todas las clases igual de bien, pues nos interesa conocer con precisión qué enfermedad posee cada paciente particular. Por esta razón, debemos utilizar técnicas de balanceo de clases. Podemos abordar este problema desde el preprocesamiento, usando técnicas de oversampling como SMOTE, o en la implementación del algoritmo, diciéndole que estamos trabajando con clases desbalanceadas (aunque esto depende de los algoritmos a utilizar, los cuales se definirán más adelante). Quisiéramos ver ambos comportamientos, entonces intentaremos explorar ambas alternativas. Esto se realiza más adelante, en las secciones de preprocesamiento y modelado (respectivamente) de este documento.\n",
    "\n",
    "Realizaremos un balanceo de datos, en el que quitaremos datos de los problemas 1, 4 y 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train= pd.concat( [Y_train, X_train], axis=1)\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat1 = data_train[data_train[\"problems_described\"] == 1]\n",
    "cat2 = data_train[data_train[\"problems_described\"] == 2]\n",
    "cat3 = data_train[data_train[\"problems_described\"] == 3]\n",
    "cat4 = data_train[data_train[\"problems_described\"] == 4]\n",
    "cat5 = data_train[data_train[\"problems_described\"] == 5]\n",
    "\n",
    "cat1 = resample(cat1, replace = False, n_samples = 1000, random_state = 0)\n",
    "cat4 = resample(cat4, replace = False, n_samples = 1000, random_state = 0)\n",
    "cat5 = resample(cat5, replace = False, n_samples = 1000, random_state = 0)\n",
    "\n",
    "data_train = pd.concat([cat1, cat2, cat3, cat4, cat5])\n",
    "#Se cuentan los valores de la variable target\n",
    "data_train[\"problems_described\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = data_train[['medical_abstracts']], data_train[['problems_described']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pasa de you're a you are \n",
    "X_train['medical_abstracts']=X_train['medical_abstracts'].apply(contractions.fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se crea nueva columna con la lista de las palabras \n",
    "X_train[\"words\"]=X_train['medical_abstracts'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def stem_and_lemmatize(words):\n",
    "    stems = stem_words(words)\n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return stems + lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"words\"]=X_train[\"words\"].apply(stem_and_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manejo de Ruido \n",
    "En esta sección se quitará o modificará todo lo que se considere como ruido:\n",
    "\n",
    "+ Caracteres no ascii\n",
    "+ Se pasará de mayusculas a minusculas\n",
    "+ Se eliminará la puntuación\n",
    "+ Se reemplazarán los números\n",
    "+ Se quitarán las palabras vacias (artículos, pronombres, preposiciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "    \n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def preprocessing(words):\n",
    "    words = to_lowercase(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_non_ascii(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"words\"]=X_train[\"words\"].apply(preprocessing)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banned = [\"and\"]\n",
    "\n",
    "def remove_banned(words):\n",
    "    \"\"\"Remove banned words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in banned:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "X_train['words'] = X_train['words'].apply(remove_banned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actualizacion de la columna words. se pone toda la lista a manera de str \n",
    "X_train['words'] = X_train['words'].apply(lambda x: ' '.join(map(str, x)))\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train= pd.concat( [Y_train, X_train], axis=1)\n",
    "data_test= pd.concat( [Y_test, X_test], axis=1)\n",
    "data_train.to_csv(\"train_Data.csv\")\n",
    "data_test.to_csv(\"test_Data.csv\")\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#los tokens (palabras) para un determinado problems_described se junten\n",
    "#Junta todas las palabras con el problema #problems_described\n",
    "def generate_docx(df, problems_described):\n",
    "    \"\"\"Gets a string of all text of a category\"\"\"\n",
    "    mylist = df[df[\"problems_described\"] == problems_described][\"words\"].tolist()\n",
    "    mydocx = ''.join(mylist)\n",
    "    return mydocx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc5=generate_docx(data_train, 5)\n",
    "doc4=generate_docx(data_train, 4)\n",
    "doc3=generate_docx(data_train, 3)\n",
    "doc2=generate_docx(data_train, 2)\n",
    "doc1=generate_docx(data_train, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Da los 50 tokens mas comunes \n",
    "def extract_keywords(text, num = 50):\n",
    "    \"\"\"Gets a dictionary with the # of ocurrences of a word\"\"\"\n",
    "    tokens = [token for token in text.split()]\n",
    "    most_common_tokens = Counter(tokens).most_common(num)\n",
    "    return dict(most_common_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_words_5= extract_keywords(doc5)\n",
    "key_words_4= extract_keywords(doc4)\n",
    "key_words_3= extract_keywords(doc3)\n",
    "key_words_2= extract_keywords(doc2)\n",
    "key_words_1= extract_keywords(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "def plot_most_common_words(mydict, problems_described):\n",
    "    df_01 = pd.DataFrame(mydict.items(),columns=[\"token\",\"count\"])\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(\"Plot of {}\".format(problems_described))\n",
    "    sns.barplot(x = 'token', y=\"count\", data = df_01)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_most_common_words(key5,\"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[\"problems_described\"]==5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante notar que almacenaremos el resultado del perfilamiento en un archivo <code>csv</code> para fácil acceso cuando tenga que ejecutarse todo el proyecto; y, segundo, nos enfocaremos en analizar únicamente aquellas funcionalidades de <code>nlp profiler</code> que sean relevantes para este proyecto. Entonces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importante: ¡Esta celda tarda mucho en ejecutarse!\n",
    "#profile_data = apply_text_profiling(diagnoses, 'medical_abstracts')\n",
    "\n",
    "# Importante: No correr esta linea si no se ha corrido la anterior.\n",
    "#profile_data.to_csv('nlp_profiler_medical_abstracts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_data = pd.read_csv('nlp_profiler_medical_abstracts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de modelos de procesamiento de textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo Multinomial Naive Bayes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
