{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "071de44f",
   "metadata": {},
   "source": [
    "### María Sofía Álvarez - Brenda Barahona - Álvaro Plata\n",
    "<h1 align='center'>Proyecto 1: Analítica de textos - LSTM</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536da7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "526dad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import datetime\n",
    "import sent2vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9a1611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868bd533",
   "metadata": {},
   "source": [
    "## Algoritmo elegido: LSTM\n",
    "\n",
    "El tercer algoritmo que usaremos para este problema será LSTM.\n",
    "\n",
    "La LSTM (Long-Short Term Memory) es un tipo de red neuronal recurrente (RNN, por sus siglas en inglés) que se desempeña mejor que las RNN tradicionales en términos de memoria [1]. Una RNN es un tipo de red neuronal que permite a las salidas de capas previas ser utilizadas como entradas, teniendo estados ocultos [2].\n",
    "\n",
    "Las LSTM tienen múltiples capas ocultas. A medida que se pasa a través de una capa, la información relevante se mantiene y la irrelevante se desecha en cada neurona (celda) individual [1]. Asimismo, las LSTM solucionan el problema de desvanecimiento de gradientes que las RNN enfrentan a menudo.\n",
    "\n",
    "Las LSTM cuentan principalmente con 3 compuertas:\n",
    "+ **FORGET Gate:** Esta compuerta es la responsable de decidir qué información se queda y cuál es irrelevante y debe descartarse. Para ello, utiliza la información que viene de la neurona anterior $h_{t-1}$ y la información de la celda actual, $x_t$. Sobre ellas, se corre una función sigmoide, $$S(x) = \\frac{1}{1 + e^{-x}},$$ tal que los datos que tiendan a 0 son descartados por la red [1].\n",
    "+ **INPUT Gate:** Esta compuerta actualiza el estado de la neurona y decide qué información es importante. Como la compuerta FORGET ayuda a descartar la información, la compuerta INPUT ayuda a encontrar la información importante y a almacenar ciertos datos relevantes en memoria. En este caso, la información de la neurona anterior, $h_{t-1}$ es pasada por una función de activación sigmoide, mientras que la información de la neurona actual $x_t$ se pasa por una función de activación de tangete hiperbólica: $$\\tanh(x) = \\frac{1 - e^{-2x}}{1 + e^{-2x}}.$$ Es importante resaltar que la función $\\tanh$ ayuda a regular la red y a reducir el sesgo de la misma [1].\n",
    "+ **OUTPUT Gate:** Tras multiplicar el estado actual de la neurona con lo que se obtiene de la compuerta FORGET, lo cual permite eliminar cierta información si la compuerta FORGET arroja pesos de 0, debe decidirse cual será el estado de la siguiente celda. La información de $h_{t-1}$ y $x_t$ se pasa a través de una función sigmoide, el cual es a su vez pasado por una función de tangente hiperbólica, y ambos resultados se mltiplican para decidir la información que llevará el estado oculto [1]. \n",
    "\n",
    "A continuación, puede verse la gráfica de las funciones $\\tanh$ y sigmoide:\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Muhammad-Hamdan-8/publication/327435257/figure/fig4/AS:742898131812354@1554132125449/Activation-Functions-ReLU-Tanh-Sigmoid.ppm\" />\n",
    "\n",
    "Note que también se encuentra la función de activación ReLU (Regularized Linear Unit), muy popular en las aplicaciones de Machine y Deep Learning.\n",
    "\n",
    "Asimismo, puede verse una representación esquemática (obtenida de [3]) de la estructura de cada una de las compuertas:\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Xuan_Hien_Le2/publication/334268507/figure/fig8/AS:788364231987201@1564972088814/The-structure-of-the-Long-Short-Term-Memory-LSTM-neural-network-Reproduced-from-Yan.png\" width=50% />\n",
    "\n",
    "En la imagen podemos ver las funciones de activación usadas en cada celda LSTM (sigmoide y tangente hiperbólica), así como sus entradas, salidas y compuertas.\n",
    "\n",
    "---\n",
    "Leamos los datos traidos de la fase anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "396a49f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_train = pd.read_csv('train_Data.csv')\n",
    "datos_test = pd.read_csv('test_Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffce0ea0",
   "metadata": {},
   "source": [
    "## Vectorización\n",
    "\n",
    "Una vez elegido el modelo, procedemos a hacer la vectorización de los datos que obtuvimos en la fase de preprocesamiento. Como dijimos, el preprocesamiento para los algoritmos basados en secuencias, como las redes neuronales, es diferente. En este caso, optaré por usar dos opciones de embedding.\n",
    "\n",
    "* **BioSentVec:** Módulo basado en las investigaciones de Zhang et. al [4]. Similar a Sent2Vec (algoritmo de embedding de Google), está basado en la librería ```fast_text```de Facebook para embeddings y clasificación de textos. La librería utiliza las base de datos de PubMed y las notas clínicas de MIMIC-III Clinical Database como corpus para entrenar una red que genera vectores de 700 dimensiones. El procedimiento para usar este modelo es largo y tedioso, pero puede encontrarse anexo a este laboratorio. Debido a que la dimensionalidad sigue siendo elevada, para evitar overfitting, solamente se usará sobre los abstracts y no sobre las entities.\n",
    "* **Keras Embedding:** Al usar redes neuronales con la librería Keras, existe una capa propia de ```Embedding```. Podemos probar esta técnica de vectorización también. Se probará sobre ambos, los abstracts y las entities.\n",
    "\n",
    "Cargamos la librería de BioWordVec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "add34179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model successfully loaded\n"
     ]
    }
   ],
   "source": [
    "model_path = 'BioSentVec_PubMed_MIMICIII-bigram_d700.bin'\n",
    "model = sent2vec.Sent2vecModel()\n",
    "try:\n",
    "    model.load_model(model_path)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print('model successfully loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b61373",
   "metadata": {},
   "source": [
    "Por la forma en la que funciona la librería, no podemos poner este paso en una Pipeline. Por lo tanto, lo que hacemos es generar los embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c98fc7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_abstracts = model.embed_sentences(datos_train['non_tokenized_abstracts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9b19330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'embedded_abstracts' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "%store embedded_abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a310e3f9",
   "metadata": {},
   "source": [
    "¡Note que la dimensionalidad de estos datos es de 700! Como vimos en el preprocesamiento, esto puede ser mucho para las entidades. Por lo tanto, para ellas, usaremos el embedding de ```sk-learn```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8837dcc3",
   "metadata": {},
   "source": [
    "## Manejo de desbalanceo de las clases\n",
    "\n",
    "Ahora, uno de los mayores problemas de la clasificación es el contexto desbalanceado. Una opción sería reducir el conjunto de datos hasta que todas las clases queden con un número de abstracts igual al tamaño de la clase de menor cantidad de abstracts. No obstante, por lo general la idea es no reducir el conjunto de datos. Otra opción, como en los algoritmos anteriores, sería usar SMOTE. No obstante, esto es computacionalmente muy costoso para la red.\n",
    "\n",
    "Lo que sí podemos hacer es considerar pesos. Así, el modelo podrá prestar mayor atención a las clases minoritarias. Para ello, usaremos la librería de <code>sk-learn</code> y lo pasaremos como un objeto al modelo que construiremos más adelante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e149612",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight(\n",
    "                class_weight = 'balanced',\n",
    "                classes = np.unique(datos_train['problems_described']), \n",
    "                y = datos_train['problems_described'])\n",
    "train_class_weights_ = dict(enumerate(class_weights, start=1))\n",
    "train_class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0ed31c",
   "metadata": {},
   "source": [
    "Podemos ver los pesos asociados a cada una de las clases, en orden ascendente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70121740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.912981455064194,\n",
       " 2: 1.93158953722334,\n",
       " 3: 1.5,\n",
       " 4: 0.9462789551503203,\n",
       " 5: 0.6011271133375078}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_class_weights_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc4ccf9",
   "metadata": {},
   "source": [
    "Como era de esperarse, recordando la gráfica del perfilamiento (mostrada más abajo), la clase con mayor peso es la 2, seguida de la clase 3. La clase mayoritaria (la 5) es la que presenta menores pesos.\n",
    "\n",
    "<img src=\"images/img_preproc.png\" width=40% />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b4cead",
   "metadata": {},
   "source": [
    "Con esto, ya podemos proceder a realizar el modelamiento. \n",
    "## Modelo LSTM\n",
    "Es importante mencionar que no hay un método estándar para la búsqueda de hiperparámetros en redes neuronales. Para ello, debemos hacer algunos *workarounds*. \n",
    "\n",
    "No obstante, antes de hacer el tuneo de hiperparámetros, en este tipo de modelos resulta más sencillo hacer un modelo base primero. Hagamos un modelo base usando el WordEmbedding de BioSentVec. \n",
    "### Modelo base: Usando BioSentVec\n",
    "Consideremos un modelo base de dos capas LSTM, acompañadas de su capa de dropout (i.e. de pérdida). De acuerdo con [1], es importante que estos modelos estén acompañados de esta capa para evitar overfitting. Finalmente, consideramos una capa densa con función de activación <code>softmax</code>, que es la función de activación que se utiliza en contextos de clasificación.\n",
    "\n",
    "Antes de eso, es importante mencionar que Keras requiere que el input esté en la forma (batch_size, timesteps, input_dim). Por lo tanto, hacemos un reshape a los arreglos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "facf94f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9600, 1, 700)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_abstracts_ = embedded_abstracts.reshape(-1, 1, embedded_abstracts.shape[1])\n",
    "embedded_abstracts_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1c17df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 5 # Cantidad de clases del problema\n",
    "# ------------------LSTM-----------------------\n",
    "# Inicializamos el modelo\n",
    "lstm_base = Sequential(name='LSTM_basico') \n",
    "# Agregamos una capa LSTM con el tamanio de entrada de los embedded abstracts y 16 neuronas en la capa\n",
    "lstm_base.add(LSTM(units=16, return_sequences=True, \n",
    "                    input_shape=(1, embedded_abstracts.shape[1])))\n",
    "# Agregamos la primera capa de dropout\n",
    "lstm_base.add(Dropout(0.2))\n",
    "# Agregamos una segunda capa LSTM con 16 neuronas\n",
    "lstm_base.add(LSTM(units=16, return_sequences=False))\n",
    "# Con su respectiva capa de dropout\n",
    "lstm_base.add(Dropout(0.2))\n",
    "# Definimos la capa de salida\n",
    "lstm_base.add(Dense(output, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8e03f1",
   "metadata": {},
   "source": [
    "Antes de ver nuestro modelo totalmente construido, conviene discutir un poco lo que acabamos de hacer. Inicializamos el modelo creando un modelo secuencial. Posteriormente, agregamos una capa LSTM con su respectiva capa de dropout. Esta es una capa de regularización, que hace que se aprenda una fracción de los pesos en la red. Para redes grandes, se recomienda un dropout $p=0.5$ , la cual corresponde a la máxima regularización. Esto también lo ajustaremos como hiperparámetro, pero es un buen punto de partida para nuestro primer modelo [5]. Agregamos otra capa de LSTM con su respectiva capa de dropout y, finalmente, diseñamos la capa de salida con 5 neuronas: pues tenemos 5 clases.\n",
    "\n",
    "Revisando la documentación de <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\"/> la capa LSTM</a>, es posible ver que hay muchos posibles hiperparámetros a configurar. No obstante, hay algunos que NO deben ser modificados, de acuerdo con la revisión teórica realizada más arriba, como la función de activación (que debe ser $\\tanh$) y la función de activación recurrente (que debe ser sigmoide). Asimismo, hay otros parámetros que están ajustados para la mayoría de aplicaciones de ML, como el inicializador del kernel (que es globot) y del kernel recurrente (que es ortogonal), por defecto. Asimismo, hay otros hiperparámetros que permiten hacer modificaciones sobre el modelo, como ```go_backwards```. Este puede ser útil para la construcción de redes recurrentes bidireccionales. Así las cosas, vemos que, dados los hiperparámetros de esta capa, en realidad los más fundamentales a determinar (y para no tener un gran costo computacional) son: \n",
    "* **units:** Indica el número de neuronas de la capa. En el modelo inicial, usamos 16. Pero puede ser cualquier número entero positivo.\n",
    "* **use_bias:** Booleano que indica si la capa usará un vector de sesgos o no.\n",
    "Estos hiperparámetros, junto con la tasa de dropout, serán tuneados más adelante. Veamos el resumen del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b0ea0dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM_basico\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_11 (LSTM)              (None, 1, 16)             45888     \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 1, 16)             0         \n",
      "                                                                 \n",
      " lstm_12 (LSTM)              (None, 16)                2112      \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 16)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 85        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48,085\n",
      "Trainable params: 48,085\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bb5ba2",
   "metadata": {},
   "source": [
    "Ahora, debemos elegir una métrica, una función de pérdida y un optimizador. En cuanto a la función de pérdida, la que se usa con frecuencia en problemas de clasificación es la de entropía cruzada. De hecho, se recomienda no cambiarla, a menos de que se tenga una razón lo suficientemente fuerte para hacerlo, pues es la función de pérdida preferida en el marco de la máxima verosimilitud. Para problemas multiclase, se utiliza entropía cruzada categórica: sparse_categorical_crossentropy [6].\n",
    "\n",
    "En el caso del optimizador, elegimos adam. Por lo general, este es el que mejores resultados presenta, de acuerdo con la literatura. Asimismo, nos quitamos de encima el ajustar un hiperparámetro extra (la tasa de aprendizaje), pues los algoritmos adaptativos como Adam van ajustando esta tasa a medida que entrenan [7]. Últimamente se ha visto que SGD, acompañado de un buen learning rate, puede arrojar resultados excelentes también. No obstante, esto implica el ajuste de un hiperparámetro que, dada la complejidad del problema, puede ser muy costosa computacionalmente.\n",
    "\n",
    "Asimismo, debemos definir la métrica que informa el éxito del modelo. En este caso elegimos la precisión como métrica, pues el usuario médico quiere clasificar tan bien como se pueda los abstracts en las enfermedades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "50b8a2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_base.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['precision'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3d9a24",
   "metadata": {},
   "source": [
    "### Control de la complejidad y los tiempos de procesamiento\n",
    "Una forma de controlar la complejidad y los tiempos de procesamiento es mediante el uso de callbacks. Estas son acciones durante las etapas del entrenamiento.\n",
    "\n",
    "De acuerdo con la documentación de Tensorflow, encontramos tres callbacks que consideramos útiles para este laboratorio. Primero, consideramos EarlyStopping. En este caso, ponemos la cantidad monitoreada como la medida que tomamos a la pérdida (val_loss) tal que, si después de 3 épocas no ha mejorado, entonces pare el entrenamiento y la actualización de los pesos. En este caso, monitoreamos sobre el error de validación.\n",
    "\n",
    "El otro callback que utilizaremos es TensorBoard. Este permite visualizar un reporte del entrenamiento, el cual nos será útil para concluir sobre el avance del modelo en función de los diferentes ciclos de aprendizaje.\n",
    "\n",
    "Finalmente, ModelCheckpoint permite guardar el modelo con mejor desempeño sobre validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4bf94969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-28 02:57:57.095415: I tensorflow/core/profiler/lib/profiler_session.cc:110] Profiler session initializing.\n",
      "2022-03-28 02:57:57.095424: I tensorflow/core/profiler/lib/profiler_session.cc:125] Profiler session started.\n",
      "2022-03-28 02:57:57.095888: I tensorflow/core/profiler/lib/profiler_session.cc:143] Profiler session tear down.\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='loss', patience=3, verbose=1, mode='auto', baseline=None)\n",
    "tensorboard_callback = TensorBoard(log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"), histogram_freq=0, write_graph=True, write_images=False, update_freq='epoch', profile_batch=2, embeddings_freq=0, embeddings_metadata=None,)\n",
    "model_checkpoint = ModelCheckpoint('base_model.h5', monitor='loss', mode='min', verbose=1, save_best_only=True)\n",
    "callbacks = [early_stopping,tensorboard_callback, model_checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7da14cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = datos_train['problems_described']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747fef4a",
   "metadata": {},
   "source": [
    "Finalmente, hacemos fit al modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2dc3427d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 864, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 957, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 438, in update_state\n        self.build(y_pred, y_true)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 358, in build\n        self._metrics = tf.__internal__.nest.map_structure_up_to(y_pred, self._get_metric_objects,\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 484, in _get_metric_objects\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 484, in <listcomp>\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 503, in _get_metric_object\n        metric_obj = metrics_mod.get(metric)\n    File \"/usr/local/lib/python3.9/site-packages/keras/metrics.py\", line 4262, in get\n        return deserialize(str(identifier))\n    File \"/usr/local/lib/python3.9/site-packages/keras/metrics.py\", line 4218, in deserialize\n        return deserialize_keras_object(\n    File \"/usr/local/lib/python3.9/site-packages/keras/utils/generic_utils.py\", line 709, in deserialize_keras_object\n        raise ValueError(\n\n    ValueError: Unknown metric function: precision. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history_base \u001b[38;5;241m=\u001b[39m \u001b[43mlstm_base\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded_abstracts_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_class_weights\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m   1148\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 864, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 957, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 438, in update_state\n        self.build(y_pred, y_true)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 358, in build\n        self._metrics = tf.__internal__.nest.map_structure_up_to(y_pred, self._get_metric_objects,\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 484, in _get_metric_objects\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 484, in <listcomp>\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 503, in _get_metric_object\n        metric_obj = metrics_mod.get(metric)\n    File \"/usr/local/lib/python3.9/site-packages/keras/metrics.py\", line 4262, in get\n        return deserialize(str(identifier))\n    File \"/usr/local/lib/python3.9/site-packages/keras/metrics.py\", line 4218, in deserialize\n        return deserialize_keras_object(\n    File \"/usr/local/lib/python3.9/site-packages/keras/utils/generic_utils.py\", line 709, in deserialize_keras_object\n        raise ValueError(\n\n    ValueError: Unknown metric function: precision. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.\n"
     ]
    }
   ],
   "source": [
    "history_base = lstm_base.fit(embedded_abstracts_, Y_train, epochs= 20, callbacks=callbacks, class_weight=train_class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1644c0a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1651060a",
   "metadata": {},
   "source": [
    "## Bibliografía\n",
    "---\n",
    "[1] https://www.analyticsvidhya.com/blog/2021/06/lstm-for-text-classification/\n",
    "\n",
    "[2] https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\n",
    "\n",
    "[3] Application of Long Short-Term Memory (LSTM) Neural Network for Flood Forecasting - Scientific Figure on ResearchGate. Available from: https://www.researchgate.net/figure/The-structure-of-the-Long-Short-Term-Memory-LSTM-neural-network-Reproduced-from-Yan_fig8_334268507 [accessed 28 Mar, 2022]\n",
    "\n",
    "[4] Zhang Y, Chen Q, Yang Z, Lin H, Lu Z. BioWordVec, improving biomedical word embeddings with subword information and MeSH. Scientific Data. 2019.\n",
    "\n",
    "[5] https://towardsdatascience.com/simplified-math-behind-dropout-in-deep-learning-6d50f3f47275\n",
    "\n",
    "[6] https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/\n",
    "\n",
    "[7] https://towardsdatascience.com/7-tips-to-choose-the-best-optimizer-47bb9c1219e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e0a6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
