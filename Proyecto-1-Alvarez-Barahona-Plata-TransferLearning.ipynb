{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07674c9c",
   "metadata": {},
   "source": [
    "### María Sofía Álvarez, Brenda Barahona, Álvaro Plata\n",
    "<h1 align='center'>Proyecto 1: Analítica de textos - Transfer Learning</h1>\n",
    "\n",
    "## Instalación de librerías\n",
    "<b><font color='blue'>Importante:</font></b> Correr antes de ejecutar el notebook. Con una única vez que se corra, basta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c7e8d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q keras-bert\n",
    "#!pip install keras-rectified-adam\n",
    "#!pip install transformers\n",
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8d44aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export NCBI_DIR='/Users/sofiaalvarezlopez/Documents/Universidad/Décimo Semestre/Inteligencia de Negocios/Proyectos/Proyecto 1/NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b57b6dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "import torch\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras_radam import RAdam\n",
    "from keras_bert import Tokenizer\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras_bert import load_trained_model_from_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc09ca7",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "El transfer learning es una de las aplicaciones de la inteligencia artificial que más fuerza ha cogido últimamente. Esta se enfoca en almacenar conocimiento obtenido al resolver un problema determinado, y reutilizarlo para aplicarlo a un problema diferente, pero relacionado.\n",
    "\n",
    "La evolución de los modelos preentrenados en los últimos años ha facilitado enormemente el desarrollo de modelado de lenguajes. Los modelos complejos de redes neuronales - incluso más profundos que la LSTM que implementamos en otro notebook -, o incluso clásicos - como Naïve-Bayes - estaban lejos de ser realmente buenos en las predicciones. Como vimos, incluso tras un arduo preprocesamiento, los resultados no son tan óptimos como esperaríamos. En este contexto, en el mundo del procesamiento del lenguaje natural, han surgido varios modelos, como los tr ansformadores, el famoso BERT (*Bidirectional Encoder Representations from Transformers*, por sus siglas en inglés), ELMo, XL-Net y Open AI GPT-2, entre otros. Incluso con poco aprendizaje por transferencia y ajustes de hiperparámetros, pueden obtenerse excelentes resultados. [8]\n",
    "\n",
    "Los avances en técnicas de transfer learning no han sido ajenos al campo de la medicina, donde han surgido modelos como SciBERT, BioBERT y ClinicalBERT. En este caso, haremos transfer learning a partir de uno de los modelos más recientes y de mayor éxito: BlueBERT. \n",
    "\n",
    "BlueBERT es un modelo de Machine Learning basado en BERT y desarrollado por Peng et. al [9], pre-entrenado sobre notas clínicas y abstracts de dos bases de datos muy conocidas en el mundo de la medicina: PubMed y MIMIC-III. \n",
    "\n",
    "A continuación, cargamos el modelo de BlueBERT y agregamos algunas capas al final (y descongelamos algunas de las últimas) con el fin de hacer transfer learning.\n",
    "\n",
    "---\n",
    "Lo primero que hacemos es cargar el modelo preentrenado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75a925a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_path = 'NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12'\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688971b2",
   "metadata": {},
   "source": [
    "Asimismo, definimos las constantes del modelo BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "154b2a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 4\n",
    "LR = 2e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec67647",
   "metadata": {},
   "source": [
    "### Preprocesamiento especial para BERT\n",
    "Primero, cargamos el conjunto de textos.  Lo primero que tenemos que notar es que este algoritmo recibe una representación completamente diferente para su entrada. Por lo tanto, debemos hacer un preprocesamiento diferente. En este caso, seguiré la aproximación de [9]. En este caso, usaremos las funciones por defecto de Keras par BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b45e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_train = pd.read_csv('train_Data.csv')\n",
    "X_train = datos_train['tokenized_abstracts']\n",
    "Y_train = datos_train['problems_described']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8f0a6b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ed57bdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gt/fh8j82dx4cj4lsdqnyn6l2fh0000gn/T/ipykernel_78019/3858949437.py:1: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  input_ids = np.array([torch.tensor(tokenizer.encode(X_train[i])).unsqueeze(0) for i in range(len(X_train))])\n",
      "/var/folders/gt/fh8j82dx4cj4lsdqnyn6l2fh0000gn/T/ipykernel_78019/3858949437.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  input_ids = np.array([torch.tensor(tokenizer.encode(X_train[i])).unsqueeze(0) for i in range(len(X_train))])\n"
     ]
    }
   ],
   "source": [
    "input_ids = np.array([torch.tensor(tokenizer.encode(X_train[i])).unsqueeze(0) for i in range(len(X_train))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ac8c78",
   "metadata": {},
   "source": [
    "Ahora, cargamos el modelo pre-entrenado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "17002cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = load_trained_model_from_checkpoint(\n",
    "  config_path,\n",
    "  checkpoint_path,\n",
    "  training=True,\n",
    "  trainable=True,\n",
    "  seq_len=SEQ_LEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a9e017",
   "metadata": {},
   "source": [
    "Debido a que ya es un modelo de Keras, podemos revisar las capas que lo conforman. Vemos que son muchísimas, y a grandes rasgos podemos ver que se construye bajo una arquitectura de Encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "737f85ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input-Token (InputLayer)       [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " Input-Segment (InputLayer)     [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " Embedding-Token (TokenEmbeddin  [(None, 128, 768),  23440896    ['Input-Token[0][0]']            \n",
      " g)                              (30522, 768)]                                                    \n",
      "                                                                                                  \n",
      " Embedding-Segment (Embedding)  (None, 128, 768)     1536        ['Input-Segment[0][0]']          \n",
      "                                                                                                  \n",
      " Embedding-Token-Segment (Add)  (None, 128, 768)     0           ['Embedding-Token[0][0]',        \n",
      "                                                                  'Embedding-Segment[0][0]']      \n",
      "                                                                                                  \n",
      " Embedding-Position (PositionEm  (None, 128, 768)    98304       ['Embedding-Token-Segment[0][0]']\n",
      " bedding)                                                                                         \n",
      "                                                                                                  \n",
      " Embedding-Dropout (Dropout)    (None, 128, 768)     0           ['Embedding-Position[0][0]']     \n",
      "                                                                                                  \n",
      " Embedding-Norm (LayerNormaliza  (None, 128, 768)    1536        ['Embedding-Dropout[0][0]']      \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Embedding-Norm[0][0]']         \n",
      " on (MultiHeadAttention)                                                                          \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-1-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Embedding-Norm[0][0]',         \n",
      " on-Add (Add)                                                     'Encoder-1-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-1-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-1-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-1-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-1-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-1-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-1-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-1-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-2-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-1-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-2-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-2-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-2-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-2-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-2-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-2-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-2-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-2-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-3-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-2-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-3-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-3-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-3-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-3-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-3-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-3-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-3-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-3-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-3-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-3-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-3-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-3-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-4-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-3-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-4-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-4-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-4-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-4-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-4-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-4-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-4-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-4-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-4-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-4-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-4-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-4-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-5-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-4-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-5-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-5-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-5-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-5-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-5-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-5-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-5-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-5-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-5-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-5-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-5-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-5-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-6-MultiHeadSelfAttentio\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-5-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-6-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-6-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-6-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-6-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-6-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-6-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-6-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-6-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-6-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-6-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-6-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-6-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-7-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-6-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-7-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-7-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-7-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-7-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-7-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-7-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-7-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-7-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-7-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-7-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-7-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-7-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-8-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-7-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-8-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-8-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-8-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-8-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-8-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-8-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-8-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-8-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-8-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-8-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-8-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-8-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-9-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-8-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-9-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-9-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-9-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-9-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-9-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-9-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-9-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-9-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-9-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-9-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-9-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    2362368     ['Encoder-9-FeedForward-Norm[0][0\n",
      " ion (MultiHeadAttention)                                        ]']                              \n",
      "                                                                                                  \n",
      " Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-10-MultiHeadSelfAttenti\n",
      " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
      "                                                                                                  \n",
      " Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-9-FeedForward-Norm[0][0\n",
      " ion-Add (Add)                                                   ]',                              \n",
      "                                                                  'Encoder-10-MultiHeadSelfAttenti\n",
      "                                                                 on-Dropout[0][0]']               \n",
      "                                                                                                  \n",
      " Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    1536        ['Encoder-10-MultiHeadSelfAttenti\n",
      " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-10-FeedForward (FeedFo  (None, 128, 768)    4722432     ['Encoder-10-MultiHeadSelfAttenti\n",
      " rward)                                                          on-Norm[0][0]']                  \n",
      "                                                                                                  \n",
      " Encoder-10-FeedForward-Dropout  (None, 128, 768)    0           ['Encoder-10-FeedForward[0][0]'] \n",
      "  (Dropout)                                                                                       \n",
      "                                                                                                  \n",
      " Encoder-10-FeedForward-Add (Ad  (None, 128, 768)    0           ['Encoder-10-MultiHeadSelfAttenti\n",
      " d)                                                              on-Norm[0][0]',                  \n",
      "                                                                  'Encoder-10-FeedForward-Dropout[\n",
      "                                                                 0][0]']                          \n",
      "                                                                                                  \n",
      " Encoder-10-FeedForward-Norm (L  (None, 128, 768)    1536        ['Encoder-10-FeedForward-Add[0][0\n",
      " ayerNormalization)                                              ]']                              \n",
      "                                                                                                  \n",
      " Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    2362368     ['Encoder-10-FeedForward-Norm[0][\n",
      " ion (MultiHeadAttention)                                        0]']                             \n",
      "                                                                                                  \n",
      " Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-11-MultiHeadSelfAttenti\n",
      " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
      "                                                                                                  \n",
      " Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-10-FeedForward-Norm[0][\n",
      " ion-Add (Add)                                                   0]',                             \n",
      "                                                                  'Encoder-11-MultiHeadSelfAttenti\n",
      "                                                                 on-Dropout[0][0]']               \n",
      "                                                                                                  \n",
      " Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    1536        ['Encoder-11-MultiHeadSelfAttenti\n",
      " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-11-FeedForward (FeedFo  (None, 128, 768)    4722432     ['Encoder-11-MultiHeadSelfAttenti\n",
      " rward)                                                          on-Norm[0][0]']                  \n",
      "                                                                                                  \n",
      " Encoder-11-FeedForward-Dropout  (None, 128, 768)    0           ['Encoder-11-FeedForward[0][0]'] \n",
      "  (Dropout)                                                                                       \n",
      "                                                                                                  \n",
      " Encoder-11-FeedForward-Add (Ad  (None, 128, 768)    0           ['Encoder-11-MultiHeadSelfAttenti\n",
      " d)                                                              on-Norm[0][0]',                  \n",
      "                                                                  'Encoder-11-FeedForward-Dropout[\n",
      "                                                                 0][0]']                          \n",
      "                                                                                                  \n",
      " Encoder-11-FeedForward-Norm (L  (None, 128, 768)    1536        ['Encoder-11-FeedForward-Add[0][0\n",
      " ayerNormalization)                                              ]']                              \n",
      "                                                                                                  \n",
      " Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    2362368     ['Encoder-11-FeedForward-Norm[0][\n",
      " ion (MultiHeadAttention)                                        0]']                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-12-MultiHeadSelfAttenti\n",
      " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
      "                                                                                                  \n",
      " Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-11-FeedForward-Norm[0][\n",
      " ion-Add (Add)                                                   0]',                             \n",
      "                                                                  'Encoder-12-MultiHeadSelfAttenti\n",
      "                                                                 on-Dropout[0][0]']               \n",
      "                                                                                                  \n",
      " Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    1536        ['Encoder-12-MultiHeadSelfAttenti\n",
      " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-12-FeedForward (FeedFo  (None, 128, 768)    4722432     ['Encoder-12-MultiHeadSelfAttenti\n",
      " rward)                                                          on-Norm[0][0]']                  \n",
      "                                                                                                  \n",
      " Encoder-12-FeedForward-Dropout  (None, 128, 768)    0           ['Encoder-12-FeedForward[0][0]'] \n",
      "  (Dropout)                                                                                       \n",
      "                                                                                                  \n",
      " Encoder-12-FeedForward-Add (Ad  (None, 128, 768)    0           ['Encoder-12-MultiHeadSelfAttenti\n",
      " d)                                                              on-Norm[0][0]',                  \n",
      "                                                                  'Encoder-12-FeedForward-Dropout[\n",
      "                                                                 0][0]']                          \n",
      "                                                                                                  \n",
      " Encoder-12-FeedForward-Norm (L  (None, 128, 768)    1536        ['Encoder-12-FeedForward-Add[0][0\n",
      " ayerNormalization)                                              ]']                              \n",
      "                                                                                                  \n",
      " MLM-Dense (Dense)              (None, 128, 768)     590592      ['Encoder-12-FeedForward-Norm[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " MLM-Norm (LayerNormalization)  (None, 128, 768)     1536        ['MLM-Dense[0][0]']              \n",
      "                                                                                                  \n",
      " Extract (Extract)              (None, 768)          0           ['Encoder-12-FeedForward-Norm[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " MLM-Sim (EmbeddingSimilarity)  (None, 128, 30522)   30522       ['MLM-Norm[0][0]',               \n",
      "                                                                  'Embedding-Token[0][1]']        \n",
      "                                                                                                  \n",
      " Input-Masked (InputLayer)      [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " NSP-Dense (Dense)              (None, 768)          590592      ['Extract[0][0]']                \n",
      "                                                                                                  \n",
      " MLM (Masked)                   (None, 128, 30522)   0           ['MLM-Sim[0][0]',                \n",
      "                                                                  'Input-Masked[0][0]']           \n",
      "                                                                                                  \n",
      " NSP (Dense)                    (None, 2)            1538        ['NSP-Dense[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,811,516\n",
      "Trainable params: 109,811,516\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cd489a",
   "metadata": {},
   "source": [
    "Es posible ver que el modelo tiene más de 109 millones de parámetros. En este caso, para poder realizar el transfer learning, hacemos ingeniería sobre las capas. Primero, extraemos las capas de entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8b567ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = bert_model.inputs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4553c816",
   "metadata": {},
   "source": [
    "Asimismo, tomamos la salida de la capa de salida (NSP-Dense):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6218d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = bert_model.get_layer('NSP-Dense').output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f330f14",
   "metadata": {},
   "source": [
    "Finalmente, aquí es donde podemos empezar a considerar el tuneo de hiperparámetros. Con el fin de no cambiar los pesos ya entrenados de BERT, lo que hacemos es, al final, decidir si pasamos el resultado que obtenemos de BERT por algunas capas profundas para obtener más características. Asimismo, también podría ajustarse el número de neuronas de cada uno de estas capas adicionales que pretendan añadirse. No obstante, esto se sale del enfoque del curso, por lo que solo implementaremos un modelo básico.\n",
    "\n",
    "Para el primer modelo básico, simplemente tomaremos el resultado de BERT y lo pasaremos por una capa densa con activación soft-max, característica de los problemas de clasificación, con 5 neuronas pues se tienen 5 clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "65009c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = keras.layers.Dense(units=5, activation='softmax')(dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01764ef0",
   "metadata": {},
   "source": [
    "Por último, compilamos nuestro modelo con las capas y vemos el resumen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0092cc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input-Token (InputLayer)       [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " Input-Segment (InputLayer)     [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " Embedding-Token (TokenEmbeddin  [(None, 128, 768),  23440896    ['Input-Token[0][0]']            \n",
      " g)                              (30522, 768)]                                                    \n",
      "                                                                                                  \n",
      " Embedding-Segment (Embedding)  (None, 128, 768)     1536        ['Input-Segment[0][0]']          \n",
      "                                                                                                  \n",
      " Embedding-Token-Segment (Add)  (None, 128, 768)     0           ['Embedding-Token[0][0]',        \n",
      "                                                                  'Embedding-Segment[0][0]']      \n",
      "                                                                                                  \n",
      " Embedding-Position (PositionEm  (None, 128, 768)    98304       ['Embedding-Token-Segment[0][0]']\n",
      " bedding)                                                                                         \n",
      "                                                                                                  \n",
      " Embedding-Dropout (Dropout)    (None, 128, 768)     0           ['Embedding-Position[0][0]']     \n",
      "                                                                                                  \n",
      " Embedding-Norm (LayerNormaliza  (None, 128, 768)    1536        ['Embedding-Dropout[0][0]']      \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Embedding-Norm[0][0]']         \n",
      " on (MultiHeadAttention)                                                                          \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-1-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Embedding-Norm[0][0]',         \n",
      " on-Add (Add)                                                     'Encoder-1-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-1-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-1-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-1-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-1-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-1-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-1-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-1-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-2-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-1-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-2-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-2-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-2-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-2-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-2-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-2-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-2-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-2-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-3-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-2-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-3-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-3-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-3-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-3-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-3-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-3-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-3-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-3-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-3-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-3-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-3-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-3-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-4-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-3-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-4-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-4-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-4-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-4-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-4-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-4-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-4-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-4-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-4-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-4-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-4-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-4-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-5-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-4-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-5-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-5-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-5-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-5-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-5-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-5-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-5-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-5-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-5-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-5-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-5-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-5-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-6-MultiHeadSelfAttentio\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-5-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-6-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-6-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-6-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-6-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-6-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-6-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-6-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-6-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-6-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-6-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-6-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-6-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-7-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-6-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-7-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-7-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-7-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-7-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-7-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-7-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-7-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-7-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-7-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-7-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-7-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-7-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-8-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-7-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-8-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-8-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-8-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-8-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-8-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-8-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-8-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-8-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-8-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-8-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-8-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-8-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-9-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-8-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-9-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-9-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-9-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-9-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-9-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-9-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-9-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-9-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-9-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-9-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-9-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    2362368     ['Encoder-9-FeedForward-Norm[0][0\n",
      " ion (MultiHeadAttention)                                        ]']                              \n",
      "                                                                                                  \n",
      " Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-10-MultiHeadSelfAttenti\n",
      " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
      "                                                                                                  \n",
      " Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-9-FeedForward-Norm[0][0\n",
      " ion-Add (Add)                                                   ]',                              \n",
      "                                                                  'Encoder-10-MultiHeadSelfAttenti\n",
      "                                                                 on-Dropout[0][0]']               \n",
      "                                                                                                  \n",
      " Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    1536        ['Encoder-10-MultiHeadSelfAttenti\n",
      " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-10-FeedForward (FeedFo  (None, 128, 768)    4722432     ['Encoder-10-MultiHeadSelfAttenti\n",
      " rward)                                                          on-Norm[0][0]']                  \n",
      "                                                                                                  \n",
      " Encoder-10-FeedForward-Dropout  (None, 128, 768)    0           ['Encoder-10-FeedForward[0][0]'] \n",
      "  (Dropout)                                                                                       \n",
      "                                                                                                  \n",
      " Encoder-10-FeedForward-Add (Ad  (None, 128, 768)    0           ['Encoder-10-MultiHeadSelfAttenti\n",
      " d)                                                              on-Norm[0][0]',                  \n",
      "                                                                  'Encoder-10-FeedForward-Dropout[\n",
      "                                                                 0][0]']                          \n",
      "                                                                                                  \n",
      " Encoder-10-FeedForward-Norm (L  (None, 128, 768)    1536        ['Encoder-10-FeedForward-Add[0][0\n",
      " ayerNormalization)                                              ]']                              \n",
      "                                                                                                  \n",
      " Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    2362368     ['Encoder-10-FeedForward-Norm[0][\n",
      " ion (MultiHeadAttention)                                        0]']                             \n",
      "                                                                                                  \n",
      " Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-11-MultiHeadSelfAttenti\n",
      " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
      "                                                                                                  \n",
      " Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-10-FeedForward-Norm[0][\n",
      " ion-Add (Add)                                                   0]',                             \n",
      "                                                                  'Encoder-11-MultiHeadSelfAttenti\n",
      "                                                                 on-Dropout[0][0]']               \n",
      "                                                                                                  \n",
      " Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    1536        ['Encoder-11-MultiHeadSelfAttenti\n",
      " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-11-FeedForward (FeedFo  (None, 128, 768)    4722432     ['Encoder-11-MultiHeadSelfAttenti\n",
      " rward)                                                          on-Norm[0][0]']                  \n",
      "                                                                                                  \n",
      " Encoder-11-FeedForward-Dropout  (None, 128, 768)    0           ['Encoder-11-FeedForward[0][0]'] \n",
      "  (Dropout)                                                                                       \n",
      "                                                                                                  \n",
      " Encoder-11-FeedForward-Add (Ad  (None, 128, 768)    0           ['Encoder-11-MultiHeadSelfAttenti\n",
      " d)                                                              on-Norm[0][0]',                  \n",
      "                                                                  'Encoder-11-FeedForward-Dropout[\n",
      "                                                                 0][0]']                          \n",
      "                                                                                                  \n",
      " Encoder-11-FeedForward-Norm (L  (None, 128, 768)    1536        ['Encoder-11-FeedForward-Add[0][0\n",
      " ayerNormalization)                                              ]']                              \n",
      "                                                                                                  \n",
      " Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    2362368     ['Encoder-11-FeedForward-Norm[0][\n",
      " ion (MultiHeadAttention)                                        0]']                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-12-MultiHeadSelfAttenti\n",
      " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
      "                                                                                                  \n",
      " Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-11-FeedForward-Norm[0][\n",
      " ion-Add (Add)                                                   0]',                             \n",
      "                                                                  'Encoder-12-MultiHeadSelfAttenti\n",
      "                                                                 on-Dropout[0][0]']               \n",
      "                                                                                                  \n",
      " Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    1536        ['Encoder-12-MultiHeadSelfAttenti\n",
      " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-12-FeedForward (FeedFo  (None, 128, 768)    4722432     ['Encoder-12-MultiHeadSelfAttenti\n",
      " rward)                                                          on-Norm[0][0]']                  \n",
      "                                                                                                  \n",
      " Encoder-12-FeedForward-Dropout  (None, 128, 768)    0           ['Encoder-12-FeedForward[0][0]'] \n",
      "  (Dropout)                                                                                       \n",
      "                                                                                                  \n",
      " Encoder-12-FeedForward-Add (Ad  (None, 128, 768)    0           ['Encoder-12-MultiHeadSelfAttenti\n",
      " d)                                                              on-Norm[0][0]',                  \n",
      "                                                                  'Encoder-12-FeedForward-Dropout[\n",
      "                                                                 0][0]']                          \n",
      "                                                                                                  \n",
      " Encoder-12-FeedForward-Norm (L  (None, 128, 768)    1536        ['Encoder-12-FeedForward-Add[0][0\n",
      " ayerNormalization)                                              ]']                              \n",
      "                                                                                                  \n",
      " Extract (Extract)              (None, 768)          0           ['Encoder-12-FeedForward-Norm[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " NSP-Dense (Dense)              (None, 768)          590592      ['Extract[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 5)            3845        ['NSP-Dense[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,191,173\n",
      "Trainable params: 109,191,173\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd64c048",
   "metadata": {},
   "source": [
    "Ahora, por defecto, BERT utiliza de optimizador el algoritmo de adam rectificado (RAdam), el cual es una variante del Adam estocástico que permite rectificar la varianza de la tasa de aprendizaje adaptativa. Asimismo, por la naturaleza de la vectorización y embedding que usa BERT, y al tratarse de un problema multiclase, usamos como función de pérdida la entropía categórica cruzada dispersa (sparse categorical cross entropy). Como métrica, volvemos a usar la precisión para ser consistentes con los modelos generados previamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "91cc9ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "      Adam(learning_rate=LR),\n",
    "      loss='sparse_categorical_crossentropy',\n",
    "      metrics=[keras.metrics.Precision(name='precision')],\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e2abaf",
   "metadata": {},
   "source": [
    "Finalmente, ha llegado la hora de correr el modelo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55edc64d",
   "metadata": {},
   "source": [
    "Asimismo, y de igual forma que el notebook anterior, volvemos a la variable objetivo una variable One-Hot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d9783c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = datos_train['problems_described']\n",
    "Y_train = keras.utils.np_utils.to_categorical(Y_train)[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b84dff6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), for inputs ['Input-Token', 'Input-Segment'] but instead got the following list of 1 arrays: [array([[tensor([[ 101, 1031, 1005,  ..., 1005, 1033,  102]])],\n       [tensor([[ 101, 1031, 1005,  ..., 1005, 1033,  102]])],\n       [tensor([[ 101, 1031, 1005,  ..., 1005, 1033,  102]])],\n       ......",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [85]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Usamos las epocas por defecto que propone BERT\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# El tamanio de los datos que se cargan en cada iteracion, propuesta por BERT\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Usamos una particion del conjunto de entrenamiento para medir la validacion\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/engine/training_v1.py:777\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_call_args(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    776\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_training_loop(x)\n\u001b[0;32m--> 777\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/engine/training_arrays_v1.py:616\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.fit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    596\u001b[0m         model,\n\u001b[1;32m    597\u001b[0m         x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    611\u001b[0m         validation_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    613\u001b[0m   batch_size \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_validate_or_infer_batch_size(batch_size,\n\u001b[1;32m    614\u001b[0m                                                    steps_per_epoch, x)\n\u001b[0;32m--> 616\u001b[0m   x, y, sample_weights \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_standardize_user_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m      \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m      \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m      \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcheck_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msteps_per_epoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    628\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m validation_data:\n\u001b[1;32m    629\u001b[0m     val_x, val_y, val_sample_weights \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_prepare_validation_data(\n\u001b[1;32m    630\u001b[0m         validation_data, batch_size, validation_steps)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/engine/training_v1.py:2335\u001b[0m, in \u001b[0;36mModel._standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m run_eagerly \u001b[38;5;129;01mand\u001b[39;00m is_build_called \u001b[38;5;129;01mand\u001b[39;00m is_compile_called \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   2332\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m is_dataset  \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(_is_symbolic_tensor(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m all_inputs)):\n\u001b[1;32m   2333\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [], [], \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2335\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_standardize_tensors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_eagerly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_eagerly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdict_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdict_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/engine/training_v1.py:2363\u001b[0m, in \u001b[0;36mModel._standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2360\u001b[0m \u001b[38;5;66;03m# Standardize the inputs.\u001b[39;00m\n\u001b[1;32m   2361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset, tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset)):\n\u001b[1;32m   2362\u001b[0m   \u001b[38;5;66;03m# TODO(fchollet): run static checks with dataset output shape(s).\u001b[39;00m\n\u001b[0;32m-> 2363\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_utils_v1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstandardize_input_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2364\u001b[0m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2365\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfeed_input_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2366\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfeed_input_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2367\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcheck_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Don't enforce the batch size.\u001b[39;49;00m\n\u001b[1;32m   2368\u001b[0m \u001b[43m      \u001b[49m\u001b[43mexception_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2370\u001b[0m \u001b[38;5;66;03m# Get typespecs for the input data and sanitize it if necessary.\u001b[39;00m\n\u001b[1;32m   2371\u001b[0m \u001b[38;5;66;03m# TODO(momernick): This should be capable of doing full input validation\u001b[39;00m\n\u001b[1;32m   2372\u001b[0m \u001b[38;5;66;03m# at all times - validate that this is so and refactor the standardization\u001b[39;00m\n\u001b[1;32m   2373\u001b[0m \u001b[38;5;66;03m# code.\u001b[39;00m\n\u001b[1;32m   2374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/engine/training_utils_v1.py:596\u001b[0m, in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(names):\n\u001b[1;32m    595\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 596\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError when checking model \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m exception_prefix \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    597\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: the list of Numpy arrays that you are passing to \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    598\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myour model is not the size the model expected. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    599\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected to see \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(names)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m array(s), \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    600\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor inputs \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(names) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but instead got the \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    601\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfollowing list of \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m arrays: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    602\u001b[0m                      \u001b[38;5;28mstr\u001b[39m(data)[:\u001b[38;5;241m200\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    603\u001b[0m   \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(names) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError when checking model \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m exception_prefix \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    605\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: you are passing a list as input to your model, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    606\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbut the model expects a list of \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(names)) \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    607\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Numpy arrays instead. The list you passed was: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    608\u001b[0m                      \u001b[38;5;28mstr\u001b[39m(data)[:\u001b[38;5;241m200\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), for inputs ['Input-Token', 'Input-Segment'] but instead got the following list of 1 arrays: [array([[tensor([[ 101, 1031, 1005,  ..., 1005, 1033,  102]])],\n       [tensor([[ 101, 1031, 1005,  ..., 1005, 1033,  102]])],\n       [tensor([[ 101, 1031, 1005,  ..., 1005, 1033,  102]])],\n       ......"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    input_ids,\n",
    "    Y_train, \n",
    "    epochs=EPOCHS, # Usamos las epocas por defecto que propone BERT\n",
    "    batch_size=BATCH_SIZE, # El tamanio de los datos que se cargan en cada iteracion, propuesta por BERT\n",
    "    validation_split=0.20, # Usamos una particion del conjunto de entrenamiento para medir la validacion\n",
    "    shuffle=True,\n",
    ")\n",
    "#model.save('20_newsgroups.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a952a243",
   "metadata": {},
   "source": [
    "## Bibliografía\n",
    "---\n",
    "[1] https://www.analyticsvidhya.com/blog/2021/06/lstm-for-text-classification/\n",
    "\n",
    "[2] https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\n",
    "\n",
    "[3] Application of Long Short-Term Memory (LSTM) Neural Network for Flood Forecasting - Scientific Figure on ResearchGate. Available from: https://www.researchgate.net/figure/The-structure-of-the-Long-Short-Term-Memory-LSTM-neural-network-Reproduced-from-Yan_fig8_334268507 [accessed 28 Mar, 2022]\n",
    "\n",
    "[4] Zhang Y, Chen Q, Yang Z, Lin H, Lu Z. BioWordVec, improving biomedical word embeddings with subword information and MeSH. Scientific Data. 2019.\n",
    "\n",
    "[5] https://towardsdatascience.com/simplified-math-behind-dropout-in-deep-learning-6d50f3f47275\n",
    "\n",
    "[6] https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/\n",
    "\n",
    "[7] https://towardsdatascience.com/7-tips-to-choose-the-best-optimizer-47bb9c1219e\n",
    "\n",
    "[8] https://medium.com/@manasmohanty/ncbi-bluebert-ncbi-bert-using-tensorflow-weights-with-huggingface-transformers-15a7ec27fc3d\n",
    "\n",
    "[9]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
