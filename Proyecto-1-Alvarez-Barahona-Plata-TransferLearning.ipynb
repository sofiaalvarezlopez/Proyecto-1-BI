{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b187fee7",
   "metadata": {},
   "source": [
    "### María Sofía Álvarez, Brenda Barahona, Álvaro Plata\n",
    "<h1 align='center'>Proyecto 1: Analítica de textos - Transfer Learning</h1>\n",
    "\n",
    "## Instalación de librerías\n",
    "<b><font color='blue'>Importante:</font></b> Correr antes de ejecutar el notebook. Con una única vez que se corra, basta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b1e81da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q keras-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1b663a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras_bert import load_trained_model_from_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbd890b",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "El transfer learning es una de las aplicaciones de la inteligencia artificial que más fuerza ha cogido últimamente. Esta se enfoca en almacenar conocimiento obtenido al resolver un problema determinado, y reutilizarlo para aplicarlo a un problema diferente, pero relacionado.\n",
    "\n",
    "La evolución de los modelos preentrenados en los últimos años ha facilitado enormemente el desarrollo de modelado de lenguajes. Los modelos complejos de redes neuronales - incluso más profundos que la LSTM que implementamos en otro notebook -, o incluso clásicos - como Naïve-Bayes - estaban lejos de ser realmente buenos en las predicciones. Como vimos, incluso tras un arduo preprocesamiento, los resultados no son tan óptimos como esperaríamos. En este contexto, en el mundo del procesamiento del lenguaje natural, han surgido varios modelos, como los tr ansformadores, el famoso BERT (*Bidirectional Encoder Representations from Transformers*, por sus siglas en inglés), ELMo, XL-Net y Open AI GPT-2, entre otros. Incluso con poco aprendizaje por transferencia y ajustes de hiperparámetros, pueden obtenerse excelentes resultados. [8]\n",
    "\n",
    "Los avances en técnicas de transfer learning no han sido ajenos al campo de la medicina, donde han surgido modelos como SciBERT, BioBERT y ClinicalBERT. En este caso, haremos transfer learning a partir de uno de los modelos más recientes y de mayor éxito: BlueBERT. \n",
    "\n",
    "BlueBERT es un modelo de Machine Learning basado en BERT y desarrollado por Peng et. al [9], pre-entrenado sobre notas clínicas y abstracts de dos bases de datos muy conocidas en el mundo de la medicina: PubMed y MIMIC-III. \n",
    "\n",
    "A continuación, cargamos el modelo de BlueBERT y agregamos algunas capas al final (y descongelamos algunas de las últimas) con el fin de hacer transfer learning.\n",
    "\n",
    "---\n",
    "Lo primero que hacemos es cargar el modelo preentrenado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c412c8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_path = 'NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12'\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c482c244",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_trained_model_from_checkpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_trained_model_from_checkpoint\u001b[49m(\n\u001b[1;32m      2\u001b[0m   config_path,\n\u001b[1;32m      3\u001b[0m   checkpoint_path,\n\u001b[1;32m      4\u001b[0m   training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m   trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m   seq_len\u001b[38;5;241m=\u001b[39mSEQ_LEN,\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_trained_model_from_checkpoint' is not defined"
     ]
    }
   ],
   "source": [
    "model = load_trained_model_from_checkpoint(\n",
    "  config_path,\n",
    "  checkpoint_path,\n",
    "  training=True,\n",
    "  trainable=True,\n",
    "  seq_len=SEQ_LEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006d96b0",
   "metadata": {},
   "source": [
    "## Bibliografía\n",
    "---\n",
    "[1] https://www.analyticsvidhya.com/blog/2021/06/lstm-for-text-classification/\n",
    "\n",
    "[2] https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\n",
    "\n",
    "[3] Application of Long Short-Term Memory (LSTM) Neural Network for Flood Forecasting - Scientific Figure on ResearchGate. Available from: https://www.researchgate.net/figure/The-structure-of-the-Long-Short-Term-Memory-LSTM-neural-network-Reproduced-from-Yan_fig8_334268507 [accessed 28 Mar, 2022]\n",
    "\n",
    "[4] Zhang Y, Chen Q, Yang Z, Lin H, Lu Z. BioWordVec, improving biomedical word embeddings with subword information and MeSH. Scientific Data. 2019.\n",
    "\n",
    "[5] https://towardsdatascience.com/simplified-math-behind-dropout-in-deep-learning-6d50f3f47275\n",
    "\n",
    "[6] https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/\n",
    "\n",
    "[7] https://towardsdatascience.com/7-tips-to-choose-the-best-optimizer-47bb9c1219e\n",
    "\n",
    "[8] https://medium.com/@manasmohanty/ncbi-bluebert-ncbi-bert-using-tensorflow-weights-with-huggingface-transformers-15a7ec27fc3d\n",
    "\n",
    "[9]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
